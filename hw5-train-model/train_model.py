import argparse
import datetime as dt
import findspark
import logging
import mlflow
import numpy as np
import os
import random

from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator, Evaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.tuning import CrossValidator
from pyspark.sql import DataFrame as SparkDataFrame, SparkSession
from pyspark.sql.functions import lit
from sklearn import metrics
from typing import List, Tuple


os.environ["GIT_PYTHON_REFRESH"] = "quiet"

dt_format = "%Y-%m-%d"
dt_format_full = f"{dt_format} %H:%M:%S"
dt_today = dt.datetime.today()
dt_today_str = dt_today.strftime(dt_format)
train_test_fraction = 0.8
random_grid_variants = 16
cross_validator_num_folds = 10
parallelism = 2

input_features = ["tx_amount", "tx_during_weekend", "tx_during_night",
                  "customer_id_nb_tx_1day_window", "customer_id_avg_amount_1day_window",
                  "customer_id_nb_tx_3day_window", "customer_id_avg_amount_3day_window",
                  "customer_id_nb_tx_7day_window", "customer_id_avg_amount_7day_window",
                  "terminal_id_nb_tx_1day_window", "terminal_id_risk_1day_window",
                  "terminal_id_nb_tx_3day_window", "terminal_id_risk_3day_window",
                  "terminal_id_nb_tx_7day_window", "terminal_id_risk_7day_window"]
output_feature = "tx_fraud"


def read_transactions_df(spark: SparkSession, dt_from: dt.datetime, dt_to: dt.datetime,
                         hdfs_dirs_input: List[str]) -> SparkDataFrame:
    logging.info(f"Reading {hdfs_dirs_input}")
    df = spark.read.parquet(*hdfs_dirs_input)
    logging.info(f"Filtering data using window {dt_from} <= dt <= {dt_to}")
    df = df.filter((df.tx_datetime >= dt_from) & (df.tx_datetime <= dt_to))
    logging.info(f"Rows to process: {df.count()}")

    return df


def get_train_test_df(df: SparkDataFrame, fraction: float) -> Tuple[SparkDataFrame, SparkDataFrame]:
    logging.info(f"Splitting train & test with {fraction=}")
    fractions = df.select(output_feature).distinct().withColumn("fraction", lit(0.8)).rdd.collectAsMap()

    train_df = df.sampleBy(output_feature, fractions)
    logging.info(f"Train: {train_df.count()=}")
    train_frauds = train_df.filter(train_df.tx_fraud == 1).count()
    train_frauds_fraction = train_frauds / train_df.count()
    logging.info(f"{train_frauds=}, {train_frauds_fraction * 100:.2f}%")

    test_df = df.subtract(train_df)
    logging.info(f"Test: {test_df.count()=}")
    test_frauds = test_df.filter(test_df.tx_fraud == 1).count()
    test_frauds_fraction = test_frauds / test_df.count()
    logging.info(f"{test_frauds=}, {test_frauds_fraction * 100:.2f}%")

    return train_df, test_df


def get_pipeline() -> Pipeline:
    # convert dataframe to vector
    vec_assembler = VectorAssembler(inputCols=input_features, outputCol="features")
    # create PCA (when we have a lot of columns or features)
    # pca = PCA(k=5, inputCol="va", outputCol="features")
    rf_classifier = RandomForestClassifier(labelCol=output_feature, featuresCol="features")
    # pipeline = Pipeline(stages=[vec_assembler, pca, rf_classifier])
    pipeline = Pipeline(stages=[vec_assembler, rf_classifier])

    return pipeline


class RandomGridBuilder:
    """
    Grid builder for random search. Sets up grids for use in CrossValidator in Spark using values
    randomly sampled from user-provided distributions.
    Distributions should be provided as lambda functions, so that the numbers are generated at call time.

    Parameters:
        num_models: Integer (Python) - number of models to generate hyperparameters for
        seed: Integer (Python) - seed (optional, default is None)

    Returns:
        param_map: list of parameter maps to use in cross validation.

    Example usage:
        from pyspark.ml.classification import LogisticRegression
        lr = LogisticRegression()
        paramGrid = RandomGridBuilder(2)\
            .addDistr(lr.regParam, lambda: np.random.rand()) \
            .addDistr(lr.maxIter, lambda : np.random.randint(10))\
            .build()

        Returns similar output as Spark ML class ParamGridBuilder and can be used in its place.
        The above paramGrid provides random hyperparameters for 2 models.
    """

    def __init__(self, num_models, seed=None):
        self._param_grid = {}
        self.num_models = num_models
        self.seed = seed

    def addDistr(self, param, distr_generator):
        """Add distribution based on dictionary generated by function passed to addDistr."""

        if 'pyspark.ml.param.Param' in str(type(param)):
            self._param_grid[param] = distr_generator
        else:
            raise TypeError('param must be an instance of Param')

        return self

    def build(self):
        param_map = []
        for n in range(self.num_models):
            if self.seed:
                # Set seeds for both numpy and random in case either is used for the random distribution
                np.random.seed(self.seed + n)
                random.seed(self.seed + n)
            param_dict = {}
            for param, distr in self._param_grid.items():
                param_dict[param] = distr()
            param_map.append(param_dict)

        return param_map


def evaluate(model, evaluator: Evaluator, train_df: SparkDataFrame, test_df: SparkDataFrame) \
        -> Tuple[SparkDataFrame, SparkDataFrame, float, float]:
    train_pred = model.transform(train_df)
    train_acc = evaluator.evaluate(train_pred)

    test_pred = model.transform(test_df)
    test_acc = evaluator.evaluate(test_pred)

    logging.info(f"Accuracy: {train_acc=:.4f}, {test_acc=:.4f}")

    return train_pred, test_pred, train_acc, test_acc


def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
    )

    parser = argparse.ArgumentParser()
    parser.add_argument("--date_from", type=str, default=dt_today_str)
    parser.add_argument("--date_to", type=str, default=dt_today_str)
    parser.add_argument("--hdfs_host", type=str, required=True)
    parser.add_argument("--hdfs_dirs_input", type=str, required=True,
                        help="HDFS folders with source data, comma separated")
    args = parser.parse_args()

    dt_from = dt.datetime.strptime(f"{args.date_from} 00:00:00", dt_format_full)
    dt_to = dt.datetime.strptime(f"{args.date_to} 23:59:59", dt_format_full)
    hdfs_dirs_input = args.hdfs_dirs_input.split(",")

    findspark.init()
    findspark.find()

    spark = (
        SparkSession.builder
        .appName("train_model")
        .master("yarn")
        .config("spark.executor.cores", "2")
        .getOrCreate()
    )
    spark.conf.set("spark.sql.repl.eagerEval.enabled", True)

    logging.info(f"{args.date_from=}")
    logging.info(f"{dt_from=}")
    logging.info(f"{args.date_to=}")
    logging.info(f"{dt_to=}")
    logging.info(f"{args.hdfs_host=}")
    logging.info(f"{args.hdfs_dirs_input=}")
    logging.info(f"{hdfs_dirs_input=}")

    transactions_df = read_transactions_df(spark, dt_from, dt_to, hdfs_dirs_input)
    train_df, test_df = get_train_test_df(transactions_df, train_test_fraction)

    pipeline = get_pipeline()
    evaluator = BinaryClassificationEvaluator(labelCol=output_feature)
    param_grid = (RandomGridBuilder(random_grid_variants)
                  .addDistr(RandomForestClassifier.maxDepth, lambda: np.random.randint(1, 16))
                  .addDistr(RandomForestClassifier.maxBins, lambda: np.random.randint(20, 100))
                  .addDistr(RandomForestClassifier.numTrees, lambda: np.random.randint(5, 50))
                  .addDistr(RandomForestClassifier.impurity, lambda: np.random.choice(['gini', 'entropy']))
                  .build())
    cross_validator = CrossValidator(estimator=pipeline,
                                     estimatorParamMaps=param_grid,
                                     evaluator=evaluator,
                                     numFolds=cross_validator_num_folds,
                                     parallelism=parallelism)

    mlflow.set_experiment("antifraud_rf")
    run_name = f"antifraud_rf_pipeline {dt.datetime.now()}"
    output_artifact = "antifraud_rf_model"

    with mlflow.start_run(run_name=run_name) as active_run:
        run_id = active_run.info.run_id

        model_cv = cross_validator.fit(train_df)
        best_model = model_cv.bestModel
        best_model_params = {
            "maxDepth": best_model.stages[-1]._java_obj.getMaxDepth(),
            "maxBins": best_model.stages[-1]._java_obj.getMaxBins(),
            "numTrees": best_model.stages[-1]._java_obj.getNumTrees(),
            "impurity": best_model.stages[-1]._java_obj.getImpurity(),
        }
        logging.info(f"{best_model_params=}")

        train_pred, test_pred, train_acc, test_acc = evaluate(model_cv, evaluator, train_df, test_df)

        # y_train = train_pred.select(output_feature).toPandas().apply(lambda x: x[0], 1).values.tolist()
        # y_train_pred = train_pred.select("prediction").toPandas().apply(lambda x: x[0], 1).values.tolist()

        y_test = test_pred.select(output_feature).toPandas().apply(lambda x: x[0], 1).values.tolist()
        y_test_pred = test_pred.select("prediction").toPandas().apply(lambda x: x[0], 1).values.tolist()

        logging.info(f"Logging params to MLflow run {run_id} ...")
        mlflow.log_params(best_model_params)
        mlflow.log_param("date_from", dt_from)
        mlflow.log_param("date_to", dt_to)
        mlflow.log_param("dataset_size", transactions_df.count())
        mlflow.log_param("train_size", train_df.count())
        mlflow.log_param("test_size", test_df.count())

        logging.info(f"Logging metrics to MLflow run {run_id} ...")
        roc_auc = round(test_acc, 4)
        precision = round(metrics.precision_score(y_test, y_test_pred), 4)
        recall = round(metrics.recall_score(y_test, y_test_pred), 4)
        logging.info(f"{roc_auc=}, {precision=}, {recall=}")

        mlflow.log_metric("roc_auc", roc_auc)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)

        logging.info("Saving model ...")
        mlflow.spark.save_model(best_model, output_artifact)

        logging.info("Exporting/logging model ...")
        mlflow.spark.log_model(best_model, output_artifact)

        logging.info("Staging model ...")
        model_version = mlflow.register_model(f"runs:/{run_id}/{output_artifact}", output_artifact)
        client = mlflow.tracking.MlflowClient()
        client.transition_model_version_stage(name=output_artifact,
                                              version=model_version.version,
                                              stage="Staging")
        logging.info("Done")

    spark.stop()


if __name__ == "__main__":
    main()
